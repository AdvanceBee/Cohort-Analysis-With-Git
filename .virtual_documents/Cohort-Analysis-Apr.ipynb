df['InvoiceMonth'] = df['InvoiceDate'].dt.to_period('M')
df['CohortMonth'] = df.groupby('CustomerID')['InvoiceDate'].transform('min').dt.to_period('M')



import pandas as pd


df = pd.read_excel('Dataset_ecommerce.xlsx')






FileNotFoundError: [Errno 2] No such file or directory: 'Dataset_ecommerce.xlsx'


import os
print(os.getcwd())



df = pd.read_excel('your_actual_filename.xlsx', engine='openpyxl')



df = pd.read_excel('your_actual_filename.xlsx', engine='openpyxl')




df = pd.read_excel('your_actual_filename.xlsx')



df = pd.read_excel('Cohort-Analysis-With-Git.xlsx')



df = pd.read_excel('C:\Users\HP\Cohort-Analysis-With-Git')


df = pd.read_excel('Cohort-Analysis-With-Git')


df = pd.read_excel('Cohort-Analysis-With-Git.xlsx', engine='openpyxl')



df = pd.read_excel('Cohort-Analysis-With-Git.xlsx')


 df = pd.read_excel('Cohort-Analysis-With-Git.xlsx' df.head()


import os
print(os.getcwd())



 df = pd.read_excel('Cohort-Analysis-With-Git.xlsx' df.head()


 df = pd.read_excel('Cohort-Analysis-With-Git\Dataset_ecommerce.xlsx' df.head()


df = pd.read_excel(r'Cohort-Analysis-With-Git\Dataset_ecommerce.xlsx', engine='openpyxl')
df.head()



df = pd.read_excel('Cohort-Analysis-With-Git\\Dataset_ecommerce.xlsx', engine='openpyxl')
df.head()



df = pd.read_excel(r'C:\Users\HP\Cohort-Analysis-With-Git\Dataset_ecommerce.xlsx', engine='openpyxl')
df.head()



df['InvoiceMonth'] = df['InvoiceDate'].dt.to_period('M')
df['CohortMonth'] = df.groupby('CustomerID')['InvoiceDate'].transform('min').dt.to_period('M')



def get_date_int(df, column):
    year = df[column].dt.year
    month = df[column].dt.month
    return year, month

invoice_year, invoice_month = get_date_int(df, 'InvoiceMonth')
cohort_year, cohort_month = get_date_int(df, 'CohortMonth')

df['CohortIndex'] = (invoice_year - cohort_year) * 12 + (invoice_month - cohort_month)



def get_date_int(df, column):
    year = df[column].dt.year
    month = df[column].dt.month
    return year, month

invoice_year, invoice_month = get_date_int(df, 'InvoiceMonth')
cohort_year, cohort_month = get_date_int(df, 'CohortMonth')

df['CohortIndex'] = (invoice_year - cohort_year) * 12 + (invoice_month - cohort_month)



# Count unique customers per CohortMonth and CohortIndex
cohort_data = df.groupby(['CohortMonth', 'CohortIndex'])['CustomerID'].nunique().reset_index()

# Pivot into a retention matrix
cohort_counts = cohort_data.pivot(index='CohortMonth', columns='CohortIndex', values='CustomerID')
cohort_counts



# Count unique customers per CohortMonth and CohortIndex
cohort_data = df.groupby(['CohortMonth', 'CohortIndex'])['CustomerID'].nunique().reset_index()

# Pivot into a retention matrix
cohort_counts = cohort_data.pivot(index='CohortMonth', columns='CohortIndex', values='CustomerID')
cohort_counts



import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(14, 6))
sns.heatmap(cohort_counts, annot=True, fmt='.0f', cmap='YlGnBu')

plt.title('Customer Retention by Cohort')
plt.xlabel('Months Since First Purchase')
plt.ylabel('Cohort Month')
plt.show()



# Divide all values in each row by the first column (month 0)
cohort_sizes = cohort_counts.iloc[:, 0]  # Initial customers per cohort
retention = cohort_counts.divide(cohort_sizes, axis=0).round(3)
retention



plt.figure(figsize=(14, 6))
sns.heatmap(retention, annot=True, fmt='.0%', cmap='BuGn')

plt.title('Monthly Retention Rate by Cohort')
plt.xlabel('Months Since First Purchase')
plt.ylabel('Cohort Month')
plt.show()



# Last transaction date in dataset
last_date = df['InvoiceDate'].max()

# Recency in days for each customer
recency_df = df.groupby('CustomerID')['InvoiceDate'].max().reset_index()
recency_df['RecencyDays'] = (last_date - recency_df['InvoiceDate']).dt.days

# Flag churned customers
recency_df['Churned'] = recency_df['RecencyDays'] > 90

recency_df.head()



# Merge churn data into main DataFrame
df = df.merge(recency_df[['CustomerID', 'Churned']], on='CustomerID', how='left')



churn_by_index = df.groupby(['CohortIndex'])['Churned'].mean().reset_index()

plt.figure(figsize=(10, 4))
sns.lineplot(data=churn_by_index, x='CohortIndex', y='Churned')
plt.title('Churn Rate by Month Since First Purchase')
plt.xlabel('Months Since First Purchase')
plt.ylabel('Churn Rate')
plt.ylim(0, 1)
plt.grid(True)
plt.show()



# Set reference date as 1 day after the last invoice
reference_date = df['InvoiceDate'].max() + pd.Timedelta(days=1)

# RFM Calculation
rfm = df.groupby('CustomerID').agg({
    'InvoiceDate': lambda x: (reference_date - x.max()).days,  # Recency
    'InvoiceNo': 'nunique',                                     # Frequency
    'TotalPrice': 'sum'                                         # Monetary
}).reset_index()

# Rename columns
rfm.columns = ['CustomerID', 'Recency', 'Frequency', 'Monetary']

rfm.head()



df['TotalPrice'] = df['Quantity'] * df['UnitPrice']



# Set reference date as 1 day after the last invoice
reference_date = df['InvoiceDate'].max() + pd.Timedelta(days=1)

# RFM Calculation
rfm = df.groupby('CustomerID').agg({
    'InvoiceDate': lambda x: (reference_date - x.max()).days,  # Recency
    'InvoiceNo': 'nunique',                                     # Frequency
    'TotalPrice': 'sum'                                         # Monetary
}).reset_index()

# Rename columns
rfm.columns = ['CustomerID', 'Recency', 'Frequency', 'Monetary']

rfm.head()



plt.figure(figsize=(15, 4))

# Recency
plt.subplot(1, 3, 1)
sns.histplot(rfm['Recency'], bins=30, kde=True)
plt.title('Recency Distribution')

# Frequency
plt.subplot(1, 3, 2)
sns.histplot(rfm['Frequency'], bins=30, kde=True)
plt.title('Frequency Distribution')

# Monetary
plt.subplot(1, 3, 3)
sns.histplot(rfm['Monetary'], bins=30, kde=True)
plt.title('Monetary Distribution')

plt.tight_layout()
plt.show()



from sklearn.preprocessing import StandardScaler

# Keep a copy of original RFM
rfm_raw = rfm.copy()

# Select RFM columns to scale
rfm_model = rfm[['Recency', 'Frequency', 'Monetary']]

# Scale using StandardScaler (mean=0, std=1)
scaler = StandardScaler()
rfm_scaled = scaler.fit_transform(rfm_model)

# Convert back to DataFrame for easy reading
rfm_scaled = pd.DataFrame(rfm_scaled, columns=['Recency', 'Frequency', 'Monetary'])
rfm_scaled.head()



# Average RFM values per cluster
cluster_profile = rfm.groupby('Cluster').agg({
    'Recency': 'mean',
    'Frequency': 'mean',
    'Monetary': 'mean',
    'CustomerID': 'count'
}).rename(columns={'CustomerID': 'Count'}).round(1)

cluster_profile



from sklearn.cluster import KMeans

# Re-run KMeans on scaled data
kmeans = KMeans(n_clusters=4, random_state=42)
clusters = kmeans.fit_predict(rfm_scaled)

# Add the cluster back to both scaled and original RFM
rfm_scaled['Cluster'] = clusters
rfm['Cluster'] = clusters



cluster_profile = rfm.groupby('Cluster').agg({
    'Recency': 'mean',
    'Frequency': 'mean',
    'Monetary': 'mean',
    'CustomerID': 'count'
}).rename(columns={'CustomerID': 'Count'}).round(1)

cluster_profile



!pip install lifetimes



from lifetimes import BetaGeoFitter
from lifetimes.utils import summary_data_from_transaction_data



# Prepare the data using the lifetimes utility
summary = summary_data_from_transaction_data(
    df, 
    customer_id_col='CustomerID',
    datetime_col='InvoiceDate',
    monetary_value_col='TotalPrice',
    observation_period_end=df['InvoiceDate'].max()
)

summary.head()



# Create and fit the BG/NBD model
bgf = BetaGeoFitter(penalizer_coef=0.001)
bgf.fit(
    frequency=summary['frequency'], 
    recency=summary['recency'], 
    T=summary['T']
)



# Predict future purchases for the next 30 days
summary['predicted_purchases'] = bgf.conditional_expected_number_of_purchases_up_to_time(
    30, 
    summary['frequency'], 
    summary['recency'], 
    summary['T']
)

summary[['frequency', 'recency', 'T', 'predicted_purchases']].head()



plt.figure(figsize=(10, 5))
sns.histplot(summary['predicted_purchases'], bins=30, kde=True)
plt.title('Predicted Purchases in the Next 30 Days')
plt.xlabel('Expected Number of Purchases')
plt.ylabel('Number of Customers')
plt.grid(True)
plt.show()



# Sort customers by most likely to purchase again
top_customers = summary.sort_values('predicted_purchases', ascending=False).head(10)
top_customers



# Merge churn flag into RFM
rfm_model = rfm.merge(recency_df[['CustomerID', 'Churned']], on='CustomerID', how='left')

# Drop CustomerID, keep features + label
X = rfm_model[['Recency', 'Frequency', 'Monetary']]
y = rfm_model['Churned']



from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)



from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

# Train the model
clf = RandomForestClassifier(random_state=42)
clf.fit(X_train, y_train)

# Predict on test set
y_pred = clf.predict(X_test)

# Evaluate
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))



import pandas as pd
import matplotlib.pyplot as plt

# Get feature importances
feature_importance = pd.Series(clf.feature_importances_, index=X.columns).sort_values(ascending=True)

# Plot
plt.figure(figsize=(8, 5))
feature_importance.plot(kind='barh', color='teal')
plt.title('Feature Importance - Churn Prediction')
plt.xlabel('Importance Score')
plt.ylabel('RFM Feature')
plt.grid(True)
plt.show()



from sklearn.metrics import mean_absolute_error, mean_squared_error
import numpy as np

# Calculate MAE and RMSE
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))

print("Mean Absolute Error (MAE):", round(mae, 4))
print("Root Mean Squared Error (RMSE):", round(rmse, 4))



# Convert to integers
y_test_int = y_test.astype(int)
y_pred_int = y_pred.astype(int)



from sklearn.metrics import mean_absolute_error, mean_squared_error
import numpy as np

# Calculate MAE and RMSE
mae = mean_absolute_error(y_test_int, y_pred_int)
rmse = np.sqrt(mean_squared_error(y_test_int, y_pred_int))

print("Mean Absolute Error (MAE):", round(mae, 4))
print("Root Mean Squared Error (RMSE):", round(rmse, 4))




